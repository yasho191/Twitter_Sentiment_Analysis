{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Disaster Tweets from Twitter\n",
    "\n",
    "Start with Importing the necessary libraries for basic data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['location'], inplace = True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['keyword'].fillna(\" \", inplace = True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tok = TweetTokenizer()\n",
    "\n",
    "def tokenize_tweets(df):\n",
    "    final_tokenized = []\n",
    "    for i in range(len(df)):\n",
    "        tokenized = tok.tokenize(df['text'][i])\n",
    "        final_tokenized.append(tokenized)\n",
    "        \n",
    "    return final_tokenized\n",
    "\n",
    "df_train['tok tweets'] = tokenize_tweets(df_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words():\n",
    "    for i in range(len(df_train)):\n",
    "        original_line = df_train['tok tweets'].iloc[i]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        final_line = [word for word in original_line if word.lower() not in stop_words]\n",
    "        df_train['tok tweets'].iloc[i] = final_line\n",
    "        # print(final_line)\n",
    "remove_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def collect_hashtags():\n",
    "    hashtags = []\n",
    "    regex = \"#(\\w+)\"\n",
    "    \n",
    "    for i in range(len(df_train)):\n",
    "        original_line = df_train['text'].iloc[i]\n",
    "        hashtag_list = re.findall(regex, original_line)\n",
    "        # print(hashtag_list)\n",
    "        for hashtag in hashtag_list:\n",
    "            # print(hashtag)\n",
    "            hashtags.append(hashtag)\n",
    "        original_line_list = df_train['tok tweets'].iloc[i]\n",
    "        for w in range(len(original_line_list)):\n",
    "            if original_line_list[w][0] == \"#\":\n",
    "                original_line_list[w] = original_line_list[w][1:]\n",
    "        df_train['tok tweets'].iloc[i] =  original_line_list\n",
    "        \n",
    "    # print(original_line_list)\n",
    "    return hashtags\n",
    "        \n",
    "hashtags = set(collect_hashtags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions():\n",
    "    regex = \"@(\\w+)\"\n",
    "    for i in range(len(df_train)):\n",
    "        original_line = df_train['text'].iloc[i]\n",
    "        original_line_list = df_train['tok tweets'].iloc[i]\n",
    "        mention_list = re.findall(regex, original_line)\n",
    "        df_train['tok tweets'].iloc[i] = [word for word in original_line_list if word[1:] not in mention_list]\n",
    "        # print(df_train['tok tweets'].iloc[i])\n",
    "        \n",
    "remove_mentions()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations():\n",
    "    # takes care of all the links as well as links are not alpha numeric\n",
    "    for i in range(len(df_train)):\n",
    "        original_line = df_train['tok tweets'].iloc[i]\n",
    "        final_line = [word for word in original_line if word.isalnum()]\n",
    "        df_train['tok tweets'].iloc[i] = final_line\n",
    "        # print(final_line)\n",
    "remove_punctuations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_shorts():\n",
    "    for i in range(len(df_train)):\n",
    "        original_line = df_train['tok tweets'].iloc[i]\n",
    "        final_line = [word for word in original_line if len(word) > 3]\n",
    "        df_train['tok tweets'].iloc[i] = final_line\n",
    "        \n",
    "remove_shorts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize():\n",
    "    lem = WordNetLemmatizer()\n",
    "    for i in range(len(df_train)):\n",
    "        original_line = df_train.loc[i, 'tok tweets']\n",
    "        for j in range(len(original_line)):\n",
    "            x = lem.lemmatize(original_line[j].lower())\n",
    "            df_train.loc[i, 'tok tweets'][j] = x\n",
    "        # print(original_line)\n",
    "lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(columns=['location'], inplace = True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['keyword'].fillna(\" \", inplace = True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
